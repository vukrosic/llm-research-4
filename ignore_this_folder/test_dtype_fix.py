#!/usr/bin/env python3
"""
Simple test script to verify dtype handling is fixed
"""

import torch
import torch.nn as nn

def test_dtype_handling():
    """Test that all classes handle dtype conversion properly"""
    print("üß™ Testing Dtype Handling")
    print("=" * 40)
    
    # Check if CUDA is available
    if not torch.cuda.is_available():
        print("‚ùå CUDA not available. Cannot test GPU operations.")
        return False
    
    print(f"‚úÖ CUDA available: {torch.cuda.get_device_name()}")
    
    try:
        # Test 1: RMSNorm
        print("\nüîç Test 1: RMSNorm Dtype Handling")
        from ignore_this_folder.llm import TritonRMSNormLayer
        
        rms_norm = TritonRMSNormLayer(128)
        rms_norm = rms_norm.to('cuda', dtype=torch.float16)
        
        x = torch.randn(4, 64, 128, device='cuda', dtype=torch.float16)
        output = rms_norm(x)
        print(f"‚úÖ RMSNorm: Input {x.dtype}, Output {output.dtype}, Weight {rms_norm.weight.dtype}")
        
        # Test 2: Rotary
        print("\nüîç Test 2: Rotary Dtype Handling")
        from ignore_this_folder.llm import Rotary
        
        rotary = Rotary(64, 128)
        rotary = rotary.to('cuda', dtype=torch.float16)
        
        q = torch.randn(4, 8, 64, 64, device='cuda', dtype=torch.float16)
        k = torch.randn(4, 8, 64, 64, device='cuda', dtype=torch.float16)
        
        q_out, k_out = rotary(q), rotary(k)
        print(f"‚úÖ Rotary: Input {q.dtype}, Output {q_out.dtype}, Buffers {rotary.cos.dtype}")
        
        # Test 3: MultiHeadAttention
        print("\nüîç Test 3: MultiHeadAttention Dtype Handling")
        from ignore_this_folder.llm import MultiHeadAttention
        
        attention = MultiHeadAttention(128, 8, 64)
        attention = attention.to('cuda', dtype=torch.float16)
        
        x = torch.randn(4, 64, 128, device='cuda', dtype=torch.float16)
        output = attention(x)
        print(f"‚úÖ Attention: Input {x.dtype}, Output {output.dtype}, Weights {attention.qkv.weight.dtype}")
        
        # Test 4: FeedForward
        print("\nüîç Test 4: FeedForward Dtype Handling")
        from ignore_this_folder.llm import FeedForward
        
        ff = FeedForward(128, 512)
        ff = ff.to('cuda', dtype=torch.float16)
        
        x = torch.randn(4, 64, 128, device='cuda', dtype=torch.float16)
        output = ff(x)
        print(f"‚úÖ FeedForward: Input {x.dtype}, Output {output.dtype}, Weights {ff.linear1.weight.dtype}")
        
        print("\nüéâ All dtype tests passed!")
        return True
        
    except Exception as e:
        print(f"‚ùå Test failed: {e}")
        import traceback
        traceback.print_exc()
        return False

if __name__ == "__main__":
    success = test_dtype_handling()
    if success:
        print("\nüöÄ Dtype handling is working correctly!")
    else:
        print("\n‚ö†Ô∏è  Dtype handling has issues. Check the error messages above.")
