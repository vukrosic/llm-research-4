#!/usr/bin/env python3
"""
Test script to diagnose checkpoint loading issues
"""

import torch
import os
import glob
import sys

def test_checkpoint_loading(checkpoint_path):
    """Test loading a checkpoint with different methods"""
    print(f"ğŸ” Testing checkpoint: {checkpoint_path}")
    
    if not os.path.exists(checkpoint_path):
        print(f"âŒ Checkpoint not found: {checkpoint_path}")
        return False
    
    # Method 1: Try standard loading
    print("\nğŸ“¥ Method 1: Standard loading...")
    try:
        checkpoint = torch.load(checkpoint_path, map_location='cpu')
        print("âœ… Standard loading successful!")
        return True
    except Exception as e:
        print(f"âŒ Standard loading failed: {e}")
    
    # Method 2: Try with weights_only=False
    print("\nğŸ“¥ Method 2: Loading with weights_only=False...")
    try:
        checkpoint = torch.load(checkpoint_path, map_location='cpu', weights_only=False)
        print("âœ… Loading with weights_only=False successful!")
        return True
    except Exception as e:
        print(f"âŒ Loading with weights_only=False failed: {e}")
    
    # Method 3: Try with pickle_module
    print("\nğŸ“¥ Method 3: Loading with pickle_module...")
    try:
        import pickle
        checkpoint = torch.load(checkpoint_path, map_location='cpu', pickle_module=pickle)
        print("âœ… Loading with pickle_module successful!")
        return True
    except Exception as e:
        print(f"âŒ Loading with pickle_module failed: {e}")
    
    # Method 4: Try to load just the model weights
    print("\nğŸ“¥ Method 4: Loading just model weights...")
    try:
        checkpoint = torch.load(checkpoint_path, map_location='cpu', weights_only=True)
        print("âœ… Loading just weights successful!")
        print("âš ï¸ Note: This only loads model weights, not config or optimizer states")
        return True
    except Exception as e:
        print(f"âŒ Loading just weights failed: {e}")
    
    return False

def list_checkpoints():
    """List available checkpoints"""
    checkpoint_dir = "checkpoints"
    if not os.path.exists(checkpoint_dir):
        print(f"âŒ No checkpoints directory found!")
        return []
    
    checkpoint_files = glob.glob(os.path.join(checkpoint_dir, "checkpoint_step_*.pt"))
    if not checkpoint_files:
        print(f"âŒ No checkpoint files found in {checkpoint_dir}/")
        return []
    
    print(f"ğŸ“ Found {len(checkpoint_files)} checkpoint(s):")
    for i, file in enumerate(checkpoint_files):
        size_mb = os.path.getsize(file) / (1024 * 1024)
        print(f"   {i+1}. {os.path.basename(file)} ({size_mb:.1f} MB)")
    
    return checkpoint_files

def main():
    print("ğŸ” Checkpoint Loading Test Script")
    print("=" * 50)
    
    # Check PyTorch version
    print(f"ğŸ Python version: {sys.version}")
    print(f"ğŸ”¥ PyTorch version: {torch.__version__}")
    print(f"ğŸ’¾ CUDA available: {torch.cuda.is_available()}")
    
    # List checkpoints
    checkpoint_files = list_checkpoints()
    
    if not checkpoint_files:
        print("\nğŸ’¡ No checkpoints to test. Run training first:")
        print("   python distributed_train.py")
        return
    
    # Test latest checkpoint
    latest_checkpoint = checkpoint_files[-1]
    print(f"\nğŸ§ª Testing latest checkpoint: {os.path.basename(latest_checkpoint)}")
    
    success = test_checkpoint_loading(latest_checkpoint)
    
    if success:
        print(f"\nâœ… Checkpoint loading successful!")
        print(f"ğŸ’¡ You can now use resume training scripts:")
        print(f"   python resume_training.py")
        print(f"   python resume_single_gpu.py")
    else:
        print(f"\nâŒ Checkpoint loading failed!")
        print(f"ğŸ’¡ This indicates a compatibility issue.")
        print(f"ğŸ’¡ Try updating the resume scripts or check PyTorch version compatibility.")

if __name__ == "__main__":
    main()
