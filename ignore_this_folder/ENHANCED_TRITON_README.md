# Enhanced Triton Kernel Integration for LLM Training

This project provides a comprehensive integration of Triton GPU kernels for high-performance LLM training, with automatic benchmarking and kernel selection.

## üöÄ Features

### Core Triton Kernels
- **RMSNorm**: Optimized layer normalization with fused forward/backward passes
- **Rotary Position Embeddings (RoPE)**: Efficient positional encoding for transformers
- **Fused SiLU-Gated MLP**: Combined feed-forward operations for better memory efficiency
- **Attention**: Fused multi-head attention with causal masking
- **Newton-Schulz Iteration**: Matrix orthogonalization for the Muon optimizer

### Smart Kernel Selection
- **Automatic Benchmarking**: Tests all kernels against PyTorch equivalents
- **Performance-Based Selection**: Automatically chooses the fastest implementation
- **Mixed Kernel Usage**: Can use Triton for some operations and PyTorch for others
- **Real-time Monitoring**: Tracks kernel performance during training

### Advanced Features
- **Memory Efficiency**: Reduced memory usage through kernel fusion
- **Mixed Precision Support**: Full float16/float32 compatibility
- **Hardware Optimization**: Automatically tuned for your GPU architecture
- **Fallback Support**: Graceful degradation when Triton is unavailable

## üì¶ Installation

### Prerequisites
- CUDA-compatible GPU (Compute Capability 7.0+)
- PyTorch 2.0+
- Python 3.8+

### Install Triton
```bash
pip install triton
```

### Verify Installation
```bash
python -c "import triton; print(f'Triton {triton.__version__} installed successfully')"
```

## üîß Configuration

### ModelConfig Options
```python
from llm import ModelConfig

config = ModelConfig(
    # ... other config options ...
    
    # Triton kernel settings
    use_triton_rmsnorm: bool = True,      # Use Triton RMSNorm
    use_triton_rotary: bool = True,       # Use Triton Rotary
    use_triton_mlp: bool = True,          # Use Triton MLP
    use_triton_attention: bool = True,    # Use Triton Attention
    use_triton_newton_schulz: bool = True, # Use Triton Newton-Schulz
    benchmark_kernels: bool = True,       # Auto-benchmark and select
)
```

### Automatic Kernel Selection
The system automatically benchmarks all available kernels and selects the best performing ones:

```python
# This happens automatically during training
kernel_selection = auto_select_kernels(config)
print(f"Selected kernels: {kernel_selection}")
```

## üß™ Testing and Benchmarking

### Quick Test
```bash
python test_enhanced_triton.py
```

### Comprehensive Benchmark
```bash
python comprehensive_benchmark.py
```

### Individual Kernel Tests
```python
from llm import benchmark_rms_norm, benchmark_rotary, benchmark_mlp

# Test specific kernels
rms_results = benchmark_rms_norm(batch_size=16, seq_len=256, d_model=384)
rotary_results = benchmark_rotary(batch_size=16, seq_len=256, n_heads=8, d_head=48)
mlp_results = benchmark_mlp(batch_size=16, seq_len=256, d_model=384, d_ff=1536)

print(f"RMSNorm speedup: {rms_results['speedup']:.2f}x")
print(f"Rotary speedup: {rotary_results['speedup']:.2f}x")
print(f"MLP speedup: {mlp_results['speedup']:.2f}x")
```

## üìä Performance Monitoring

### Training Logs
The system automatically logs kernel selections to Weights & Biases:

```python
# Automatic logging during training
wandb.run.summary["use_triton_rmsnorm"] = config.use_triton_rmsnorm
wandb.run.summary["use_triton_rotary"] = config.use_triton_rotary
wandb.run.summary["use_triton_mlp"] = config.use_triton_mlp
wandb.run.summary["use_triton_attention"] = config.use_triton_attention
wandb.run.summary["use_triton_newton_schulz"] = config.use_triton_newton_schulz
```

### Manual Benchmarking
```python
from llm import manual_kernel_benchmark

# Run manual benchmark
manual_kernel_benchmark()
```

## üèóÔ∏è Architecture

### Kernel Integration Flow
```
Input Data ‚Üí Auto-Benchmark ‚Üí Kernel Selection ‚Üí Model Creation ‚Üí Training
     ‚Üì              ‚Üì              ‚Üì              ‚Üì           ‚Üì
  PyTorch      Performance     Best Kernel   Mixed Model   Monitor
  Fallback       Testing       Selection     (Triton +     Performance
                                        PyTorch)
```

### Kernel Selection Strategy
1. **Warmup Phase**: Run each kernel multiple times to stabilize performance
2. **Benchmark Phase**: Measure execution time across multiple iterations
3. **Selection Phase**: Choose the fastest kernel for each operation
4. **Fallback Phase**: Use PyTorch if Triton kernels fail or are slower

## üîç Kernel Details

### RMSNorm Kernel
- **Operation**: `y = x * weight / sqrt(mean(x¬≤) + eps)`
- **Optimization**: Fused forward/backward, optimized memory access
- **Block Size**: Automatically tuned for GPU architecture

### Rotary Kernel
- **Operation**: Applies rotation matrices to query/key vectors
- **Optimization**: Pre-computed cos/sin tables, vectorized operations
- **Memory**: Reduced intermediate tensor allocations

### Fused MLP Kernel
- **Operation**: `y = (x @ W1) * silu(x @ W3) @ W2`
- **Optimization**: Single kernel for gate computation
- **Memory**: Eliminates intermediate activations

### Attention Kernel
- **Operation**: Multi-head attention with causal masking
- **Optimization**: Fused QKV operations, optimized softmax
- **Memory**: Reduced memory bandwidth usage

### Newton-Schulz Kernel
- **Operation**: Matrix orthogonalization via iterative refinement
- **Optimization**: GPU-optimized matrix operations
- **Convergence**: 5-10 iterations for high precision

## üö® Troubleshooting

### Common Issues

#### Triton Import Error
```bash
# Error: ModuleNotFoundError: No module named 'triton'
pip install triton
```

#### CUDA Memory Issues
```python
# Reduce batch size or sequence length
config.batch_size = 8  # Reduce from 16
config.max_seq_len = 256  # Reduce from 512
```

#### Kernel Compilation Errors
```python
# Check GPU compatibility
python -c "import torch; print(torch.cuda.get_device_properties(0))"
```

#### Performance Degradation
```python
# Disable Triton for specific operations
config.use_triton_rmsnorm = False
config.use_triton_attention = False
```

### Debug Mode
```python
# Enable verbose logging
import logging
logging.basicConfig(level=logging.DEBUG)

# Test individual kernels
from llm import benchmark_rms_norm
results = benchmark_rms_norm(16, 256, 384, verbose=True)
```

## üìà Performance Expectations

### Typical Speedups
- **RMSNorm**: 1.2x - 2.5x
- **Rotary**: 1.1x - 1.8x
- **MLP**: 1.3x - 3.0x
- **Attention**: 1.1x - 2.0x
- **Newton-Schulz**: 1.5x - 4.0x

### Memory Reductions
- **Overall**: 10-25% reduction in peak memory usage
- **Intermediate Tensors**: 30-50% reduction
- **Gradient Memory**: 15-30% reduction

### Hardware Considerations
- **Best Performance**: RTX 4090, A100, H100
- **Good Performance**: RTX 3090, RTX 4080, V100
- **Limited Performance**: GTX 1080, RTX 2070

## üî¨ Advanced Usage

### Custom Kernel Development
```python
import triton
import triton.language as tl

@triton.jit
def custom_kernel(x_ptr, y_ptr, n_elements, BLOCK_SIZE: tl.constexpr):
    pid = tl.program_id(0)
    block_start = pid * BLOCK_SIZE
    offsets = block_start + tl.arange(0, BLOCK_SIZE)
    mask = offsets < n_elements
    
    x = tl.load(x_ptr + offsets, mask=mask, other=0.0)
    y = x * 2.0  # Your custom operation
    tl.store(y_ptr + offsets, y, mask=mask)
```

### Kernel Profiling
```python
import torch.profiler

with torch.profiler.profile(
    activities=[torch.profiler.ProfilerActivity.CPU, torch.profiler.ProfilerActivity.CUDA],
    record_shapes=True
) as prof:
    output = model(input_data)
    
print(prof.key_averages().table(sort_by="cuda_time_total"))
```

### Mixed Precision Training
```python
from torch.cuda.amp import autocast, GradScaler

scaler = GradScaler()

with autocast():
    output = model(input_data)
    loss = criterion(output, target)

scaler.scale(loss).backward()
scaler.step(optimizer)
scaler.update()
```

## üìö References

- [Triton Documentation](https://triton-lang.org/)
- [PyTorch Performance Tuning](https://pytorch.org/tutorials/recipes/recipes/tuning_guide.html)
- [CUDA Programming Guide](https://docs.nvidia.com/cuda/)
- [Transformer Architecture](https://arxiv.org/abs/1706.03762)

## ü§ù Contributing

1. Fork the repository
2. Create a feature branch
3. Add tests for new kernels
4. Ensure all benchmarks pass
5. Submit a pull request

## üìÑ License

This project is licensed under the MIT License - see the LICENSE file for details.

## üôè Acknowledgments

- Triton team for the excellent GPU kernel framework
- PyTorch team for the robust deep learning framework
- NVIDIA for CUDA and GPU technology
- The open-source AI community for inspiration and feedback
