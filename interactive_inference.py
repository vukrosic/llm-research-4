import torch
import torch.nn.functional as F
import os
import glob
from llm import MinimalLLM, ModelConfig, TextTokenDataset
from data_server import CentralDataServer
import argparse

def find_latest_checkpoint(checkpoint_dir="checkpoints"):
    """Find the latest checkpoint file"""
    if not os.path.exists(checkpoint_dir):
        return None
    
    checkpoint_files = glob.glob(os.path.join(checkpoint_dir, "checkpoint_step_*.pt"))
    if not checkpoint_files:
        return None
    
    # Extract step numbers and find the latest
    step_numbers = []
    for file in checkpoint_files:
        try:
            step = int(file.split("_step_")[1].split(".pt")[0])
            step_numbers.append((step, file))
        except:
            continue
    
    if not step_numbers:
        return None
    
    # Return the latest checkpoint
    latest_step, latest_file = max(step_numbers, key=lambda x: x[0])
    print(f"ğŸ“ Found latest checkpoint: {latest_file} (step {latest_step})")
    return latest_file

def load_model_from_checkpoint(checkpoint_path, device):
    """Load model from checkpoint"""
    print(f"ğŸ”„ Loading model from {checkpoint_path}")
    
    try:
        # Try loading with weights_only=False for PyTorch 2.6+ compatibility
        checkpoint = torch.load(checkpoint_path, map_location=device, weights_only=False)
    except Exception as e:
        print(f"âš ï¸ Standard loading failed: {e}")
        print("ğŸ”„ Trying with weights_only=False for PyTorch 2.6+ compatibility...")
        checkpoint = torch.load(checkpoint_path, map_location=device, weights_only=False)
    
    config = checkpoint['config']
    model_state_dict = checkpoint['model_state_dict']
    
    # Create model
    model = MinimalLLM(config)
    model.load_state_dict(model_state_dict)
    model = model.to(device)
    model.eval()
    
    print(f"âœ… Model loaded successfully!")
    print(f"   Step: {checkpoint['step']}")
    print(f"   Epoch: {checkpoint['epoch']}")
    print(f"   Loss: {checkpoint['loss']:.4f}")
    
    return model, config

def generate_text(model, tokenizer, prompt, max_length=100, temperature=0.8, top_k=50, top_p=0.9):
    """Generate text using the model"""
    model.eval()
    
    # Tokenize prompt
    input_ids = tokenizer.encode(prompt, add_special_tokens=False, return_tensors="pt")
    input_ids = input_ids.to(next(model.parameters()).device)
    
    # Ensure input is 2D: [seq_len] -> [1, seq_len]
    if input_ids.dim() == 1:
        input_ids = input_ids.unsqueeze(0)
    
    generated = input_ids.clone()
    
    # Debug info
    print(f"ğŸ” Input shape: {input_ids.shape}")
    print(f"ğŸ” Generated shape: {generated.shape}")
    
    with torch.no_grad():
        for step in range(max_length):
            # Get model predictions
            outputs = model(generated)
            
            # Validate output shape
            if outputs.dim() != 3:
                raise ValueError(f"Expected 3D output from model, got {outputs.dim()}D")
            if outputs.size(0) != 1:
                raise ValueError(f"Expected batch size 1, got {outputs.size(0)}")
            
            next_token_logits = outputs[0, -1, :] / temperature
            
            # Apply top-k and top-p filtering
            if top_k > 0:
                top_k_logits, top_k_indices = torch.topk(next_token_logits, top_k)
                next_token_logits = torch.full_like(next_token_logits, float('-inf'))
                next_token_logits[top_k_indices] = top_k_logits
            
            if top_p < 1.0:
                sorted_logits, sorted_indices = torch.sort(next_token_logits, descending=True)
                cumulative_probs = torch.cumsum(F.softmax(sorted_logits, dim=-1), dim=-1)
                sorted_indices_to_remove = cumulative_probs > top_p
                sorted_indices_to_remove[1:] = sorted_indices_to_remove[:-1].clone()
                sorted_indices_to_remove[0] = 0
                indices_to_remove = sorted_indices[sorted_indices_to_remove]
                next_token_logits[indices_to_remove] = float('-inf')
            
            # Sample next token
            probs = F.softmax(next_token_logits, dim=-1)
            next_token = torch.multinomial(probs, num_samples=1)
            
            # Debug tensor shapes (only first few steps to avoid spam)
            if step < 3:
                print(f"ğŸ” Step {step}: Generated shape: {generated.shape}, Next token shape: {next_token.shape}")
            
            # Append to generated sequence
            # next_token is [1], we need [1, 1] to concatenate with generated [1, seq_len]
            next_token = next_token.unsqueeze(1)  # [1] -> [1, 1]
            generated = torch.cat([generated, next_token], dim=1)
            
            # Stop if EOS token is generated
            if next_token.item() == tokenizer.eos_token_id:
                break
    
    # Decode generated text
    generated_text = tokenizer.decode(generated[0], skip_special_tokens=True)
    return generated_text

def interactive_mode(model, tokenizer, device):
    """Interactive text generation mode"""
    print("\nğŸ­ Interactive Text Generation Mode")
    print("Type 'quit' to exit, 'help' for options")
    print("=" * 50)
    
    while True:
        try:
            prompt = input("\nğŸ’¬ Enter your prompt: ").strip()
            
            if prompt.lower() == 'quit':
                print("ğŸ‘‹ Goodbye!")
                break
            elif prompt.lower() == 'help':
                print("\nğŸ“– Available commands:")
                print("  quit - Exit the program")
                print("  help - Show this help message")
                print("  clear - Clear the screen")
                print("\nğŸ“ Generation parameters:")
                print("  max_length: 100 tokens")
                print("  temperature: 0.8 (creativity)")
                print("  top_k: 50 (diversity)")
                print("  top_p: 0.9 (nucleus sampling)")
                continue
            elif prompt.lower() == 'clear':
                os.system('clear' if os.name == 'posix' else 'cls')
                continue
            elif not prompt:
                continue
            
            print(f"\nğŸ¤– Generating text...")
            print(f"ğŸ“ Prompt: {prompt}")
            
            # Generate text
            try:
                generated_text = generate_text(
                    model, tokenizer, prompt, 
                    max_length=100, temperature=0.8, top_k=50, top_p=0.9
                )
                
                print(f"\nâœ¨ Generated text:")
                print(f"{generated_text}")
                print(f"\n" + "="*50)
            except Exception as e:
                print(f"âŒ Generation error: {e}")
                print(f"ğŸ’¡ This might be a model compatibility issue.")
                print(f"ğŸ’¡ Try a different prompt or check the model checkpoint.")
            
        except KeyboardInterrupt:
            print("\n\nğŸ‘‹ Goodbye!")
            break
        except Exception as e:
            print(f"âŒ Error: {e}")

def main():
    parser = argparse.ArgumentParser(description="Interactive inference with trained LLM")
    parser.add_argument("--checkpoint", type=str, help="Path to specific checkpoint file")
    parser.add_argument("--device", type=str, default="auto", help="Device to use (auto, cpu, cuda)")
    parser.add_argument("--max-length", type=int, default=100, help="Maximum generation length")
    parser.add_argument("--temperature", type=float, default=0.8, help="Generation temperature")
    parser.add_argument("--top-k", type=int, default=50, help="Top-k sampling")
    parser.add_argument("--top-p", type=float, default=0.9, help="Top-p sampling")
    
    args = parser.parse_args()
    
    # Device setup
    if args.device == "auto":
        device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    else:
        device = torch.device(args.device)
    
    print(f"ğŸ” Using device: {device}")
    
    # Find checkpoint
    if args.checkpoint:
        checkpoint_path = args.checkpoint
        if not os.path.exists(checkpoint_path):
            print(f"âŒ Checkpoint not found: {checkpoint_path}")
            return
    else:
        checkpoint_path = find_latest_checkpoint()
        if not checkpoint_path:
            print("âŒ No checkpoints found! Please train the model first.")
            print("ğŸ’¡ Run: python distributed_train.py")
            return
    
    # Load data server to get tokenizer
    print("ğŸ“š Loading tokenizer...")
    server = CentralDataServer()
    data = server.load_and_cache_data()
    tokenizer = data['tokenizer']
    
    # Load model
    try:
        model, config = load_model_from_checkpoint(checkpoint_path, device)
    except Exception as e:
        print(f"âŒ Error loading checkpoint: {e}")
        print("ğŸ’¡ This might be a PyTorch 2.6+ compatibility issue.")
        print("ğŸ’¡ Try updating the checkpoint loading code or use an older PyTorch version.")
        return
    
    # Print model info
    print(f"\nğŸ“Š Model Information:")
    print(f"   Architecture: {config.d_model}d, {config.n_layers}L, {config.n_heads}H")
    print(f"   Vocabulary size: {config.vocab_size}")
    print(f"   Max sequence length: {config.max_seq_len}")
    
    # Test model with simple input
    print(f"\nğŸ§ª Testing model with simple input...")
    try:
        test_input = torch.tensor([[1, 2, 3]], device=device)  # Simple test sequence
        with torch.no_grad():
            test_output = model(test_input)
        print(f"âœ… Model test successful! Output shape: {test_output.shape}")
    except Exception as e:
        print(f"âŒ Model test failed: {e}")
        print(f"ğŸ’¡ The model may have compatibility issues.")
        return
    
    # Start interactive mode
    interactive_mode(model, tokenizer, device)

if __name__ == "__main__":
    main()
